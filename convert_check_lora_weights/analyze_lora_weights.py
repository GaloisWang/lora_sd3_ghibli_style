import os
from safetensors.torch import load_file

def find_safetensors_files(root_dir, recursive=True):
    """
    æŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸‹çš„ Safetensors æ–‡ä»¶ï¼ˆ.safetensors æˆ– .pt.safetensors ç­‰ï¼‰
    
    :param root_dir: æ ¹ç›®å½•è·¯å¾„
    :param recursive: æ˜¯å¦é€’å½’æŸ¥æ‰¾å­ç›®å½•ï¼Œé»˜è®¤ True
    :return: ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    safetensors_files = []
    # æ”¯æŒçš„æ‰©å±•åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    extensions = ('.safetensors', '.SafeTensors', '.SAFETENSORS')
    
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦åŒ¹é…
            if filename.lower().endswith(extensions):
                file_path = os.path.join(dirpath, filename)
                safetensors_files.append(file_path)
        # éé€’å½’æ—¶ä»…éå†å½“å‰ç›®å½•
        if not recursive:
            break
    return safetensors_files


def analyze_lora_weights(lora_path):
    print(f"ğŸ” Loading LoRA weights from: {lora_path}")
    state_dict = load_file(lora_path)

    lora_keys = [k for k in state_dict.keys() if 'lora' in k.lower()]
    if not lora_keys:
        print("âŒ No LoRA layers found in the model.")
        return

    print(f"âœ… Found {len(lora_keys)} LoRA layers. Showing statistics:")
    print("-" * 60)
    for name in lora_keys:
        param = state_dict[name]
        mean = param.mean().item()
        std = param.std().item()
        max_val = param.max().item()
        min_val = param.min().item()
        print(f"{name:<50} | mean={mean:.6f} std={std:.6f} max={max_val:.6f} min={min_val:.6f}")
    print("-" * 60)

if __name__ == "__main__":
    lora_paths_dir = "/home/lora_sd3_train_logs_0602_myscript_rank16e5/lora_final/"
    lora_paths = find_safetensors_files(lora_paths_dir)
    if len(lora_paths) == 1:
        print("æ‰¾åˆ°loraç›®å½•ä¸‹çš„æƒé‡æ–‡ä»¶,è·¯å¾„ä¸º:{0}".format(lora_paths[0]))
    else:
        print("lora_paths_dirä¸‹çš„æƒé‡æ–‡ä»¶ä¸ªæ•°ä¸ä¸º1")
        exit(0)

    analyze_lora_weights(lora_paths[0])
