import os
import gc
import torch
import logging
from pathlib import Path
from tqdm import tqdm
from PIL import Image
from diffusers import StableDiffusion3Pipeline
from contextlib import contextmanager
from typing import Optional, List, Tuple

import os
import json
from safetensors import safe_open

# ==== æ—¥å¿—é…ç½® ====
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ==== é…ç½®è·¯å¾„ ====
PATHS = {
    "real_style_folder": "/root/autodl-tmp/HuggingFace_Datasets/ItzLoghotXD--Ghibli-filtered-deduplicate/",
    "captions_folder": "/root/autodl-tmp/HuggingFace_Datasets/ItzLoghotXD--Ghibli/captions_gstyle",
    "official_model_path": "/root/autodl-tmp/models/stabilityai--stable-diffusion-3-medium-diffusers/",
    "lora_model_dir": "/home/lora_sd3_train_logs_fulldata_060216e5/lora_final/",
    "output_official": "/home/sd3_lora_compare/output_official_111",
    "output_lora": "/home/sd3_lora_compare/output_finetuned_fulldata_060216e5",
}

# ==== ç”Ÿæˆé…ç½® ====
CONFIG = {
    "guidance_scale": 7.5,
    "num_inference_steps": 50,
    "use_deterministic": True,
    "base_seed": 42,
    # å†…å­˜ä¼˜åŒ–é…ç½®
    "enable_memory_efficient_attention": True,
    "enable_cpu_offload": True,
}


class SD3BatchGenerator:
    def __init__(self):
        self.device = self._get_device()
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32

        # åˆ›å»ºè¾“å‡ºç›®å½•
        for output_path in [PATHS["output_official"], PATHS["output_lora"]]:
            Path(output_path).mkdir(parents=True, exist_ok=True)

        # æ˜¾å­˜ä¿¡æ¯
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPU Memory: {total_memory:.1f} GB")

    def _get_device(self) -> str:
        """æ™ºèƒ½è®¾å¤‡æ£€æµ‹"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    @contextmanager
    def memory_cleanup(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºå†…å­˜æ¸…ç†"""
        try:
            yield
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

    def _load_pipeline(
        self, model_path: str, enable_lora: bool = False
    ) -> StableDiffusion3Pipeline:
        """ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„LoRAåŠ è½½pipeline"""
        model_type = "LoRA" if enable_lora else "Official"
        logger.info(f"ğŸ”„ Loading {model_type} SD3 model...")

        pipe = StableDiffusion3Pipeline.from_pretrained(
            model_path, torch_dtype=self.dtype
        ).to(self.device)

        # å¦‚æœæ˜¯LoRAæ¨¡å‹ï¼Œä½¿ç”¨æ”¹è¿›çš„åŠ è½½æ–¹æ³•
        if enable_lora:
            logger.info("ğŸ”„ Loading LoRA weights with auto-detection...")
            pipe.load_lora_weights(PATHS["lora_model_dir"])

        pipe.set_progress_bar_config(disable=True)

        return pipe

    def _clear_model(self, pipe: StableDiffusion3Pipeline):
        """å®Œå…¨æ¸…é™¤æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
        logger.info("ğŸ§¹ Clearing model from memory...")
        del pipe

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # è·å–æ¸…ç†åçš„æ˜¾å­˜ä¿¡æ¯
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            logger.info(
                f"ğŸ“Š GPU Memory after cleanup - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB"
            )

        gc.collect()
        logger.info("âœ… Memory cleanup completed")

    def _validate_prompt(self, prompt: str) -> str:
        """éªŒè¯å’Œæ¸…ç†prompt"""
        prompt = " ".join(prompt.split())
        if len(prompt) > 512:
            logger.debug(f"Processing long prompt with {len(prompt)} characters")
        return prompt

    def generate_image(
        self, pipe: StableDiffusion3Pipeline, prompt: str, seed: int
    ) -> Optional[Image.Image]:
        """ç”Ÿæˆå•å¼ å›¾ç‰‡"""
        try:
            prompt = self._validate_prompt(prompt)
            generator = torch.Generator(device=self.device).manual_seed(seed)

            result = pipe(
                prompt=prompt,
                guidance_scale=CONFIG["guidance_scale"],
                num_inference_steps=CONFIG["num_inference_steps"],
                generator=generator,
            )
            return result.images[0]

        except torch.cuda.OutOfMemoryError:
            logger.error("âŒ CUDA out of memory during generation")
            torch.cuda.empty_cache()
            return None
        except Exception as e:
            logger.error(f"âŒ Generation failed: {e}")
            return None

    def get_tasks(self) -> List[Tuple[str, str, int, Path, Path]]:
        """è·å–éœ€è¦å¤„ç†çš„ä»»åŠ¡åˆ—è¡¨"""
        image_extensions = {".png", ".jpg", ".jpeg", ".bmp", ".tiff"}
        tasks = []

        # éå†å›¾ç‰‡æ–‡ä»¶
        for file_path in Path(PATHS["real_style_folder"]).iterdir():
            if file_path.suffix.lower() not in image_extensions:
                continue

            base_name = file_path.stem

            # æ£€æŸ¥captionæ˜¯å¦å­˜åœ¨
            caption_path = Path(PATHS["captions_folder"]) / f"{base_name}.txt"
            if not caption_path.exists():
                logger.warning(f"âš ï¸ Caption not found for {base_name}")
                continue

            # è¯»å–caption
            try:
                with open(caption_path, "r", encoding="utf-8") as f:
                    prompt = f.read().strip()
            except Exception as e:
                logger.error(f"âŒ Failed to read caption for {base_name}: {e}")
                continue

            # ç”Ÿæˆç§å­
            if CONFIG["use_deterministic"]:
                seed = CONFIG["base_seed"] + hash(base_name) % 1000000
            else:
                seed = CONFIG["base_seed"]

            # è¾“å‡ºè·¯å¾„
            official_path = Path(PATHS["output_official"]) / f"{base_name}.png"
            lora_path = Path(PATHS["output_lora"]) / f"{base_name}.png"

            tasks.append((base_name, prompt, seed, official_path, lora_path))

        return sorted(tasks)

    def save_image(self, image: Image.Image, output_path: Path) -> bool:
        """ä¿å­˜å›¾ç‰‡"""
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            image.save(output_path, optimize=True, quality=95)
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to save image to {output_path}: {e}")
            return False

    def generate_batch(
        self, tasks: List[Tuple[str, str, int, Path, Path]], model_type: str
    ):
        """æ‰¹é‡ç”Ÿæˆå›¾ç‰‡"""
        if model_type == "official":
            logger.info("ğŸ¨ Starting Official Model Generation Phase")
            pipe = self._load_pipeline(PATHS["official_model_path"], enable_lora=False)
            output_index = 3  # official_path
        else:
            logger.info("ğŸ¨ Starting LoRA Model Generation Phase")
            pipe = self._load_pipeline(PATHS["official_model_path"], enable_lora=True)
            output_index = 4  # lora_path

        successful = 0
        failed = 0
        skipped = 0

        # è¿‡æ»¤éœ€è¦ç”Ÿæˆçš„ä»»åŠ¡
        tasks_to_process = []
        for task in tasks:
            output_path = task[output_index]
            if output_path.exists():
                skipped += 1
            else:
                tasks_to_process.append(task)

        logger.info(
            f"ğŸ“‹ {model_type.upper()} - Total: {len(tasks)}, To process: {len(tasks_to_process)}, Skipped: {skipped}"
        )

        if not tasks_to_process:
            logger.info(f"âœ… All {model_type} images already exist, skipping...")
            self._clear_model(pipe)
            return successful, failed, skipped

        # ç”Ÿæˆå›¾ç‰‡
        progress_bar = tqdm(tasks_to_process, desc=f"Generating {model_type} images")

        for base_name, prompt, seed, official_path, lora_path in progress_bar:
            output_path = official_path if model_type == "official" else lora_path

            # ç”Ÿæˆå›¾ç‰‡
            image = self.generate_image(pipe, prompt, seed)

            if image is not None:
                if self.save_image(image, output_path):
                    successful += 1
                else:
                    failed += 1
            else:
                failed += 1

            progress_bar.set_postfix(
                {"Success": successful, "Failed": failed, "Skipped": skipped}
            )

            # å®šæœŸæ¸…ç†å†…å­˜
            if (successful + failed) % 10 == 0:
                torch.cuda.empty_cache()

        # æ¸…ç†æ¨¡å‹
        self._clear_model(pipe)

        logger.info(
            f"âœ… {model_type.upper()} Generation Complete - Success: {successful}, Failed: {failed}, Skipped: {skipped}"
        )
        return successful, failed, skipped

    def generate_comparison_images(self):
        """ä¸»è¦çš„å›¾ç‰‡ç”Ÿæˆå‡½æ•° - æ‰¹å¤„ç†æ¨¡å¼"""
        # è·å–æ‰€æœ‰ä»»åŠ¡
        tasks = self.get_tasks()
        logger.info(f"ğŸ“‹ Total tasks found: {len(tasks)}")

        if not tasks:
            logger.warning("âš ï¸ No valid tasks found!")
            return

        total_stats = {"successful": 0, "failed": 0, "skipped": 0}

        # # ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆå®˜æ–¹æ¨¡å‹å›¾ç‰‡
        # logger.info("=" * 60)
        # logger.info("ğŸš€ PHASE 1: Official Model Generation")
        # logger.info("=" * 60)

        # success, failed, skipped = self.generate_batch(tasks, "official")
        # total_stats['successful'] += success
        # total_stats['failed'] += failed
        # total_stats['skipped'] += skipped

        # # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ¸…ç†å®Œæˆ
        # logger.info("â³ Waiting for memory cleanup...")
        # torch.cuda.empty_cache()
        # gc.collect()

        # ç¬¬äºŒé˜¶æ®µï¼šç”ŸæˆLoRAæ¨¡å‹å›¾ç‰‡
        logger.info("=" * 60)
        logger.info("ğŸš€ PHASE 2: LoRA Model Generation")
        logger.info("=" * 60)

        success, failed, skipped = self.generate_batch(tasks, "lora")
        total_stats["successful"] += success
        total_stats["failed"] += failed
        total_stats["skipped"] += skipped

        # æœ€ç»ˆç»Ÿè®¡
        logger.info("=" * 60)
        logger.info("ğŸ“Š FINAL SUMMARY")
        logger.info("=" * 60)
        logger.info(
            f"""
                    Total Tasks: {len(tasks)}
                    Total Successful: {total_stats['successful']}
                    Total Failed: {total_stats['failed']}
                    Total Skipped: {total_stats['skipped']}
                    Seed Strategy: {'Deterministic per image' if CONFIG['use_deterministic'] else 'Fixed seed'}
                    """
        )


def main():
    """ä¸»å‡½æ•°"""
    generator = SD3BatchGenerator()

    try:
        logger.info("ğŸ¯ Starting SD3 Batch Generation")
        logger.info("ğŸ“ Strategy: Sequential model loading (Official â†’ LoRA)")

        # ç”Ÿæˆå¯¹æ¯”å›¾ç‰‡
        generator.generate_comparison_images()

        logger.info("ğŸ‰ All generations completed successfully!")

    except KeyboardInterrupt:
        logger.info("â¸ï¸ Generation interrupted by user")
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {e}")
        raise
    finally:
        # æœ€ç»ˆæ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        logger.info("ğŸ§¹ Final cleanup completed")


if __name__ == "__main__":
    main()
