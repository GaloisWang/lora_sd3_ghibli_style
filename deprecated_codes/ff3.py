import os
import gc
import torch
import logging
from pathlib import Path
from tqdm import tqdm
from PIL import Image
from diffusers import StableDiffusion3Pipeline
from contextlib import contextmanager
from typing import Optional, List, Tuple

def enhanced_lora_test(official_model_path, lora_model_dir, test_prompt="a cat"):
    """å¢å¼ºçš„LoRAæµ‹è¯•ï¼ŒåŒ…å«å¤šç§æ£€æµ‹æ–¹æ³•"""
    
    print("ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
    pipe = StableDiffusion3Pipeline.from_pretrained(
        official_model_path,
        torch_dtype=torch.float16,
        variant="fp16",
        safety_checker=None,
        requires_safety_checker=False
    ).to("cuda" if torch.cuda.is_available() else "cpu")
    
    print("ğŸ“Š åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ£€æŸ¥åˆå§‹çŠ¶æ€...")
    
    # è®°å½•åŸå§‹æƒé‡ç”¨äºå¯¹æ¯”
    original_weights = {}
    for name, param in pipe.transformer.named_parameters():
        if any(target in name for target in ["to_q", "to_k", "to_v", "to_out"]):
            original_weights[name] = param.clone().detach()
    
    print(f"è®°å½•äº† {len(original_weights)} ä¸ªåŸå§‹æƒé‡ç”¨äºå¯¹æ¯”")
    
    print(f"ğŸ”„ åŠ è½½LoRAæƒé‡: {lora_model_dir}")
    try:
        pipe.load_lora_weights(lora_model_dir)
        print("âœ… LoRAæƒé‡åŠ è½½å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
    except Exception as e:
        print(f"âŒ LoRAåŠ è½½å¤±è´¥: {str(e)}")
        return False


if __name__ == "__main__":
    config = {
        'official_model_path': "/root/autodl-tmp/models/stabilityai--stable-diffusion-3-medium-diffusers/",
        'lora_model_dir': "/home/fixed_sd3_lorav2/"
    }
    
    # è¿è¡ŒéªŒè¯
    is_loaded = enhanced_lora_test(
        config['official_model_path'],
        config['lora_model_dir']
    )