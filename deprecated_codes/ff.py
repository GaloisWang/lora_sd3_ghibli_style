import torch
from diffusers import StableDiffusion3Pipeline
import os

def comprehensive_lora_verification(official_model_path, lora_model_dir):
    """å…¨é¢éªŒè¯LoRAæƒé‡åŠ è½½"""
    print("ğŸš€ å¼€å§‹åŠ è½½å®˜æ–¹æ¨¡å‹...")
    pipe = StableDiffusion3Pipeline.from_pretrained(
        official_model_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        variant="fp16"
    ).to("cpu")
    print("âœ… å®˜æ–¹æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è®°å½•å¤šä¸ªå±‚çš„åŸå§‹æƒé‡
    original_weights = {}
    layers_to_check = [
        ("transformer.transformer_blocks.0.attn.to_q", pipe.transformer.transformer_blocks[0].attn.to_q.weight),
        ("transformer.transformer_blocks.0.attn.to_k", pipe.transformer.transformer_blocks[0].attn.to_k.weight),
        ("transformer.transformer_blocks.0.attn.to_v", pipe.transformer.transformer_blocks[0].attn.to_v.weight),
        ("transformer.transformer_blocks.1.attn.to_q", pipe.transformer.transformer_blocks[1].attn.to_q.weight),
    ]
    
    for name, weight in layers_to_check:
        original_weights[name] = weight.data.clone()
        print(f"ğŸ“‹ è®°å½•åŸå§‹æƒé‡ {name}: {weight.data[0, :3].tolist()}")
    
    print(f"\nğŸ”„ åŠ è½½LoRAæƒé‡: {lora_model_dir}")
    try:
        # åŠ è½½LoRAæƒé‡
        pipe.load_lora_weights(lora_model_dir)
        print("âœ… LoRAæƒé‡æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # å¯ç”¨LoRAé€‚é…å™¨ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
        if hasattr(pipe, 'enable_lora'):
            pipe.enable_lora()
            print("âœ… LoRAé€‚é…å™¨å·²å¯ç”¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰LoRAé€‚é…å™¨è¢«åŠ è½½
        if hasattr(pipe, 'get_active_adapters'):
            active_adapters = pipe.get_active_adapters()
            print(f"ğŸ” æ´»è·ƒçš„é€‚é…å™¨: {active_adapters}")
        
    except Exception as e:
        print(f"âŒ LoRAåŠ è½½å¤±è´¥: {str(e)}")
        return False
    
    # æ£€æŸ¥æƒé‡å˜åŒ–
    print(f"\nğŸ“Š æ£€æŸ¥æƒé‡å˜åŒ–...")
    any_change = False
    
    for name, original_weight in original_weights.items():
        current_weight = None
        try:
            # é‡æ–°è·å–å½“å‰æƒé‡
            if "transformer_blocks.0.attn.to_q" in name:
                current_weight = pipe.transformer.transformer_blocks[0].attn.to_q.weight.data
            elif "transformer_blocks.0.attn.to_k" in name:
                current_weight = pipe.transformer.transformer_blocks[0].attn.to_k.weight.data
            elif "transformer_blocks.0.attn.to_v" in name:
                current_weight = pipe.transformer.transformer_blocks[0].attn.to_v.weight.data
            elif "transformer_blocks.1.attn.to_q" in name:
                current_weight = pipe.transformer.transformer_blocks[1].attn.to_q.weight.data
                
            if current_weight is not None:
                weight_diff = torch.abs(original_weight - current_weight).mean().item()
                max_diff = torch.abs(original_weight - current_weight).max().item()
                
                print(f"ğŸ“ˆ {name}:")
                print(f"   åŸå§‹: {original_weight[0, :3].tolist()}")
                print(f"   å½“å‰: {current_weight[0, :3].tolist()}")
                print(f"   å¹³å‡å·®å¼‚: {weight_diff:.8f}")
                print(f"   æœ€å¤§å·®å¼‚: {max_diff:.8f}")
                
                if weight_diff > 1e-6:  # é™ä½é˜ˆå€¼
                    any_change = True
                    
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å±‚ {name} æ—¶å‡ºé”™: {str(e)}")
    
    # é¢å¤–æ£€æŸ¥ï¼šéªŒè¯LoRAå‚æ•°
    print(f"\nğŸ” æ£€æŸ¥LoRAå‚æ•°çŠ¶æ€...")
    lora_found = False
    
    # æ£€æŸ¥transformerä¸­çš„LoRAå‚æ•°
    try:
        for name, module in pipe.transformer.named_modules():
            if hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):
                lora_found = True
                print(f"âœ… å‘ç°LoRAå‚æ•°åœ¨: transformer.{name}")
                
                # æ£€æŸ¥LoRAå‚æ•°æ˜¯å¦éé›¶
                if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                    lora_a_norm = torch.norm(module.lora_A.weight).item()
                    lora_b_norm = torch.norm(module.lora_B.weight).item()
                    print(f"   LoRA_Aæƒé‡èŒƒæ•°: {lora_a_norm:.6f}")
                    print(f"   LoRA_Bæƒé‡èŒƒæ•°: {lora_b_norm:.6f}")
                    if lora_a_norm > 0 or lora_b_norm > 0:
                        any_change = True
                    break
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥transformer LoRAå‚æ•°æ—¶å‡ºé”™: {str(e)}")
    
    # ä¹Ÿæ£€æŸ¥text_encoderä¸­çš„LoRAå‚æ•°
    try:
        if hasattr(pipe, 'text_encoder') and pipe.text_encoder is not None:
            for name, module in pipe.text_encoder.named_modules():
                if hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):
                    lora_found = True
                    print(f"âœ… å‘ç°LoRAå‚æ•°åœ¨: text_encoder.{name}")
                    break
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥text_encoder LoRAå‚æ•°æ—¶å‡ºé”™: {str(e)}")
    
    # æ£€æŸ¥LoRAé€‚é…å™¨çŠ¶æ€
    try:
        if hasattr(pipe, '_lora_loaded_modules'):
            loaded_modules = getattr(pipe, '_lora_loaded_modules', {})
            if loaded_modules:
                print(f"âœ… å·²åŠ è½½çš„LoRAæ¨¡å—: {list(loaded_modules.keys())}")
                lora_found = True
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥LoRAé€‚é…å™¨çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
    
    if not lora_found:
        print("âŒ æœªæ‰¾åˆ°LoRAå‚æ•°ï¼Œå¯èƒ½åŠ è½½å¤±è´¥")
    
    return any_change or lora_found

def check_lora_files(lora_dir):
    """æ£€æŸ¥LoRAç›®å½•ä¸­çš„æ–‡ä»¶"""
    print(f"\nğŸ“ æ£€æŸ¥LoRAç›®å½•: {lora_dir}")
    
    if not os.path.exists(lora_dir):
        print("âŒ LoRAç›®å½•ä¸å­˜åœ¨")
        return False
        
    files = os.listdir(lora_dir)
    print(f"ğŸ“‹ ç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„LoRAæ–‡ä»¶
    lora_files = [f for f in files if f.endswith(('.safetensors', '.bin', '.pt', '.pth'))]
    if lora_files:
        print(f"âœ… æ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶: {lora_files}")
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
        return False

if __name__ == "__main__":
    config = {
        'official_model_path': "/root/autodl-tmp/models/stabilityai--stable-diffusion-3-medium-diffusers/",
        'lora_model_dir': "/home/models/erikhsos-campusbier-sd3-lora/"
    }
    
    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶
    files_ok = check_lora_files(config['lora_model_dir'])
    
    if files_ok:
        # è¿è¡ŒéªŒè¯
        print("\n" + "="*60)
        print("ğŸ” å¼€å§‹å…¨é¢LoRAéªŒè¯...")
        print("="*60)
        
        is_loaded = comprehensive_lora_verification(
            config['official_model_path'],
            config['lora_model_dir']
        )
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\n" + "="*60)
        print(f"ğŸ¯ æœ€ç»ˆéªŒè¯ç»“æœ: {'âœ… æˆåŠŸ' if is_loaded else 'âŒ å¤±è´¥'}")
        print("="*60)
        
        if not is_loaded:
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥LoRAæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("2. å°è¯•ä¸åŒçš„LoRAåŠ è½½æ–¹å¼")
            print("3. ç¡®è®¤LoRAæ˜¯å¦ä¸SD3å…¼å®¹")
            print("4. æ£€æŸ¥LoRAçš„ç¼©æ”¾å‚æ•°")
    else:
        print("âŒ æ— æ³•ç»§ç»­éªŒè¯ï¼Œè¯·æ£€æŸ¥LoRAæ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")