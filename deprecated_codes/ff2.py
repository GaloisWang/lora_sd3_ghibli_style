import os
import gc
import torch
import logging
from pathlib import Path
from tqdm import tqdm
from PIL import Image
from diffusers import StableDiffusion3Pipeline
from contextlib import contextmanager
from typing import Optional, List, Tuple

import os
import json
from safetensors import safe_open



def verify_lora_layers(official_model_path, lora_model_dir):
    """é€šè¿‡æ£€æŸ¥LoRAå±‚å­˜åœ¨æ€§éªŒè¯åŠ è½½"""
    # 1. åŠ è½½æ¨¡å‹
    pipe = StableDiffusion3Pipeline.from_pretrained(
        official_model_path,
        torch_dtype=torch.float32,
        variant="fp16",
        safety_checker=None,
        requires_safety_checker=False
    ).to("cpu")
    
    # 2. åŠ è½½LoRAæƒé‡
    pipe.load_lora_weights(lora_model_dir)
    
    # 3. æ£€æŸ¥æ˜¯å¦å­˜åœ¨LoRAç‰¹å®šå±‚
    lora_found = False
    for name, module in pipe.transformer.named_modules():
        if "lora" in name.lower():
            print(f"ğŸ” æ£€æµ‹åˆ°LoRAå±‚: {name}")
            lora_found = True
            # æ£€æŸ¥æƒé‡æ˜¯å¦éé›¶
            for param in module.parameters():
                if param.abs().sum().item() > 1e-6:
                    print(f"  æƒé‡éªŒè¯: éé›¶å€¼å­˜åœ¨ ({param.abs().sum().item():.6f})")
                    return True
    
    if not lora_found:
        print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•LoRAå±‚")
        return False
    return True

if __name__ == "__main__":
    config = {
        'official_model_path': "/root/autodl-tmp/models/stabilityai--stable-diffusion-3-medium-diffusers/",
        'lora_model_dir': "/home/other_peoples/"
    }
    
    # è¿è¡ŒéªŒè¯
    is_loaded = verify_lora_layers(
        config['official_model_path'],
        config['lora_model_dir']
    )