import torch
import torchvision.transforms as T
import os
from PIL import Image
from tqdm import tqdm
import lpips
from pytorch_fid import fid_score
from transformers import CLIPProcessor, CLIPModel
import torch.nn.functional as F
import re
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å®šä¹‰æ¨¡å‹æƒé‡æ–‡ä»¶çš„å®Œæ•´æœ¬åœ°è·¯å¾„
local_hf_model_dir = "/home/models/openai--clip-vit-base-patch32/openai--clip-vit-base-patch32/" 
if not os.path.exists(local_hf_model_dir):
    raise FileNotFoundError(f"CLIPæ¨¡å‹æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {local_hf_model_dir}ã€‚è¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶åã€‚")

try:
    # ä½¿ç”¨æ­£ç¡®çš„CLIPæ¨¡å‹å’Œå¤„ç†å™¨
    clip_model = CLIPModel.from_pretrained(local_hf_model_dir)
    clip_processor = CLIPProcessor.from_pretrained(local_hf_model_dir)
    
    print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ åŠ è½½CLIPæ¨¡å‹æ—¶å‡ºé”™: {e}")
    print("è¯·ç¡®ä¿ç›®å½•åŒ…å«æ‰€æœ‰å¿…éœ€æ–‡ä»¶ (config.json, pytorch_model.bin, tokenizerç­‰)")
    raise

# å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
clip_model = clip_model.to(device)
clip_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# åŠ è½½ LPIPS æ¨¡å‹
lpips_model = lpips.LPIPS(net='alex').to(device)

def extract_key_phrases(text, max_phrases=10):
    """ä»é•¿æ–‡æœ¬ä¸­æå–å…³é”®çŸ­è¯­"""
    # ç§»é™¤å¸¸è§çš„åœç”¨è¯å’Œè¿æ¥è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}
    
    # æå–å½¢å®¹è¯+åè¯çš„ç»„åˆ
    important_patterns = [
        r'\b(?:beautiful|stunning|gorgeous|elegant|serene|mystical|magical|ancient|traditional|modern|colorful|vibrant|peaceful|dramatic)\s+\w+',
        r'\b(?:ghibli|studio|anime|manga|japanese|traditional|style|art|painting|illustration)',
        r'\b(?:forest|mountain|castle|village|garden|temple|house|building|landscape|nature|sky|cloud|water|tree|flower)',
        r'\b(?:character|girl|boy|woman|man|person|people|figure)',
        r'\b(?:green|blue|red|yellow|purple|pink|orange|white|black|golden|silver)\s+\w+',
    ]
    
    key_phrases = []
    text_lower = text.lower()
    
    # æå–åŒ¹é…çš„å…³é”®çŸ­è¯­
    for pattern in important_patterns:
        matches = re.findall(pattern, text_lower)
        key_phrases.extend(matches)
    
    # ç§»é™¤é‡å¤å¹¶é™åˆ¶æ•°é‡
    unique_phrases = list(dict.fromkeys(key_phrases))[:max_phrases]
    
    if unique_phrases:
        return ', '.join(unique_phrases)
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®çŸ­è¯­ï¼Œè¿”å›å‰å‡ ä¸ªé‡è¦è¯æ±‡
        words = text_lower.split()
        important_words = [w for w in words if w not in stop_words and len(w) > 2]
        return ' '.join(important_words[:15])

def split_and_average_embeddings(text, images, max_length=75):
    """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œåˆ†åˆ«è®¡ç®—embeddingsç„¶åå¹³å‡"""
    # æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if not sentences:
        sentences = [text]
    
    # å¦‚æœå•ä¸ªå¥å­ä»ç„¶å¤ªé•¿ï¼ŒæŒ‰é€—å·åˆ†å‰²
    final_segments = []
    for sentence in sentences:
        if len(clip_processor.tokenizer.encode(sentence)) <= max_length:
            final_segments.append(sentence)
        else:
            # æŒ‰é€—å·åˆ†å‰²é•¿å¥å­
            parts = sentence.split(',')
            current_part = ""
            for part in parts:
                test_part = current_part + ("," if current_part else "") + part.strip()
                if len(clip_processor.tokenizer.encode(test_part)) <= max_length:
                    current_part = test_part
                else:
                    if current_part:
                        final_segments.append(current_part)
                    current_part = part.strip()
            if current_part:
                final_segments.append(current_part)
    
    # å¦‚æœè¿˜æ˜¯æœ‰è¿‡é•¿çš„ç‰‡æ®µï¼Œä½¿ç”¨å…³é”®çŸ­è¯­æå–
    processed_segments = []
    for segment in final_segments:
        if len(clip_processor.tokenizer.encode(segment)) > max_length:
            segment = extract_key_phrases(segment)
        processed_segments.append(segment)
    
    # è®¡ç®—æ‰€æœ‰ç‰‡æ®µçš„æ–‡æœ¬embeddings
    text_embeddings = []
    
    for segment in processed_segments:
        try:
            inputs = clip_processor(
                text=[segment], 
                images=images, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=77
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = clip_model(**inputs)
                text_emb = outputs.text_embeds  # ä½¿ç”¨text_embedsè€Œä¸æ˜¯logits
                text_embeddings.append(text_emb)
                
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡æœ¬ç‰‡æ®µæ—¶å‡ºé”™: {e}")
            continue
    
    if not text_embeddings:
        raise RuntimeError("æ— æ³•å¤„ç†ä»»ä½•æ–‡æœ¬ç‰‡æ®µ")
    
    # å¹³å‡æ‰€æœ‰æ–‡æœ¬embeddings
    avg_text_embedding = torch.stack(text_embeddings).mean(dim=0)
    
    # è®¡ç®—å›¾åƒembeddings
    image_inputs = clip_processor(images=images, return_tensors="pt")
    image_inputs = {k: v.to(device) for k, v in image_inputs.items()}
    
    with torch.no_grad():
        image_outputs = clip_model(**image_inputs)
        image_embeddings = image_outputs.image_embeds
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = F.cosine_similarity(avg_text_embedding, image_embeddings, dim=-1)
    
    return similarity

def compute_clip_score_with_long_text(image_folder, prompt_folder, method="key_phrases"):
    """
    æ”¯æŒé•¿æ–‡æœ¬çš„CLIPåˆ†æ•°è®¡ç®—
    method: "key_phrases" | "split_average" | "sliding_window"
    """
    images, prompts, names = load_images_and_prompts(image_folder, prompt_folder)
    
    if len(images) == 0:
        raise RuntimeError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒç”¨äºCLIPè¯„åˆ†")
    
    similarities = []
    long_text_count = 0
    
    print(f"ğŸ”„ ä½¿ç”¨ {method} æ–¹æ³•è®¡ç®—CLIPåˆ†æ•°...")
    
    for i, (image, prompt, name) in enumerate(tqdm(zip(images, prompts, names), total=len(images))):
        try:
            token_length = len(clip_processor.tokenizer.encode(prompt))
            
            if token_length <= 75:
                # çŸ­æ–‡æœ¬ç›´æ¥å¤„ç†
                inputs = clip_processor(
                    text=[prompt], 
                    images=[image], 
                    return_tensors="pt", 
                    padding=True,
                    truncation=True,
                    max_length=77
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = clip_model(**inputs)
                    similarity = outputs.logits_per_image[0, 0]
                    similarities.append(similarity.cpu().item())
            else:
                # é•¿æ–‡æœ¬å¤„ç†
                long_text_count += 1
                
                if method == "key_phrases":
                    # æ–¹æ³•1ï¼šæå–å…³é”®çŸ­è¯­
                    short_prompt = extract_key_phrases(prompt)
                    inputs = clip_processor(
                        text=[short_prompt], 
                        images=[image], 
                        return_tensors="pt", 
                        padding=True,
                        truncation=True,
                        max_length=77
                    )
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = clip_model(**inputs)
                        similarity = outputs.logits_per_image[0, 0]
                        similarities.append(similarity.cpu().item())
                
                elif method == "split_average":
                    # æ–¹æ³•2ï¼šåˆ†å‰²æ–‡æœ¬å¹¶å¹³å‡embeddings
                    similarity = split_and_average_embeddings(prompt, [image])
                    similarities.append(similarity.cpu().item())
                
                elif method == "sliding_window":
                    # æ–¹æ³•3ï¼šæ»‘åŠ¨çª—å£ï¼Œå–æœ€é«˜ç›¸ä¼¼åº¦
                    words = prompt.split()
                    max_similarity = -1
                    window_size = 50  # å¤§çº¦å¯¹åº”75ä¸ªtokens
                    
                    for start in range(0, len(words), window_size//2):  # 50%é‡å 
                        window_text = ' '.join(words[start:start+window_size])
                        if len(clip_processor.tokenizer.encode(window_text)) > 77:
                            continue
                            
                        inputs = clip_processor(
                            text=[window_text], 
                            images=[image], 
                            return_tensors="pt", 
                            padding=True,
                            truncation=True,
                            max_length=77
                        )
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = clip_model(**inputs)
                            sim = outputs.logits_per_image[0, 0].cpu().item()
                            max_similarity = max(max_similarity, sim)
                    
                    if max_similarity > -1:
                        similarities.append(max_similarity)
                    else:
                        # å›é€€åˆ°å…³é”®çŸ­è¯­æ–¹æ³•
                        short_prompt = extract_key_phrases(prompt)
                        inputs = clip_processor(
                            text=[short_prompt], 
                            images=[image], 
                            return_tensors="pt", 
                            padding=True,
                            truncation=True,
                            max_length=77
                        )
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = clip_model(**inputs)
                            similarity = outputs.logits_per_image[0, 0]
                            similarities.append(similarity.cpu().item())
                
        except Exception as e:
            print(f"âš ï¸  å¤„ç† {name} æ—¶å‡ºé”™: {e}")
            continue
    
    if not similarities:
        raise RuntimeError("âŒ æ— æ³•è®¡ç®—ä»»ä½•æœ‰æ•ˆçš„CLIPåˆ†æ•°")
    
    avg_similarity = sum(similarities) / len(similarities)
    print(f"âœ… CLIPå¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} (åŸºäº {len(similarities)} ä¸ªæ ·æœ¬)")
    print(f"ğŸ“Š å…¶ä¸­ {long_text_count} ä¸ªé•¿æ–‡æœ¬ä½¿ç”¨äº† {method} æ–¹æ³•å¤„ç†")
    
    return avg_similarity

def analyze_prompt_lengths(prompt_folder):
    """åˆ†ææç¤ºæ–‡æœ¬çš„é•¿åº¦åˆ†å¸ƒ"""
    print("ğŸ” åˆ†ææç¤ºæ–‡æœ¬é•¿åº¦...")
    
    lengths = []
    long_prompts = []
    
    prompt_files = [f for f in os.listdir(prompt_folder) if f.endswith('.txt')]
    
    for fname in prompt_files:
        try:
            with open(os.path.join(prompt_folder, fname), 'r', encoding='utf-8') as f:
                text = f.read().strip()
                token_length = len(clip_processor.tokenizer.encode(text))
                lengths.append(token_length)
                
                if token_length > 77:
                    long_prompts.append((fname, token_length, text[:100] + "..."))
                    
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶ {fname} æ—¶å‡ºé”™: {e}")
            continue
    
    if lengths:
        avg_length = sum(lengths) / len(lengths)
        max_length = max(lengths)
        min_length = min(lengths)
        over_limit = len([l for l in lengths if l > 77])
        
        print(f"ğŸ“Š æç¤ºæ–‡æœ¬ç»Ÿè®¡:")
        print(f"   â€¢ æ€»æ•°: {len(lengths)}")
        print(f"   â€¢ å¹³å‡é•¿åº¦: {avg_length:.1f} tokens")
        print(f"   â€¢ æœ€å¤§é•¿åº¦: {max_length} tokens")
        print(f"   â€¢ æœ€å°é•¿åº¦: {min_length} tokens")
        print(f"   â€¢ è¶…è¿‡77 tokens: {over_limit} ä¸ª ({over_limit/len(lengths)*100:.1f}%)")
        
        if long_prompts:
            print(f"\nğŸ“ éƒ¨åˆ†è¿‡é•¿çš„æç¤ºæ–‡æœ¬ç¤ºä¾‹:")
            for fname, length, preview in long_prompts[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   â€¢ {fname}: {length} tokens")
                print(f"     '{preview}'")
    
    return lengths

def load_images_and_prompts(image_folder, prompt_folder):
    """åŠ è½½å›¾åƒå’Œå¯¹åº”çš„æç¤ºæ–‡æœ¬"""
    images, prompts, names = [], [], []
    
    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    for fname in sorted(image_files):
        base_name = os.path.splitext(fname)[0]
        prompt_path = os.path.join(prompt_folder, f"{base_name}.txt")
        
        if not os.path.exists(prompt_path):
            print(f"âš ï¸  æç¤ºæ–‡ä»¶æœªæ‰¾åˆ°: {fname}, è·³è¿‡è¯¥æ–‡ä»¶")
            continue
            
        try:
            # åŠ è½½å›¾åƒ
            img_path = os.path.join(image_folder, fname)
            img = Image.open(img_path).convert("RGB")
            images.append(img)
            
            # åŠ è½½å¯¹åº” prompt
            with open(prompt_path, 'r', encoding='utf-8') as pf:
                prompt_text = pf.read().strip()
                prompts.append(prompt_text)
                
            names.append(fname)
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {fname} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(images)} ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹")
    return images, prompts, names

def load_images_for_lpips(image_folder):
    """ä¸ºLPIPSåŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸ºå¼ é‡"""
    images = []
    transform = T.Compose([
        T.Resize((256, 256)),  # LPIPSé€šå¸¸ä½¿ç”¨256x256
        T.ToTensor(),
        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # å½’ä¸€åŒ–åˆ°[-1,1]
    ])
    
    image_files = sorted([f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    
    for fname in image_files:
        try:
            img_path = os.path.join(image_folder, fname)
            img = Image.open(img_path).convert("RGB")
            img_tensor = transform(img)
            images.append(img_tensor)
        except Exception as e:
            print(f"âš ï¸  å¤„ç†LPIPSå›¾åƒ {fname} æ—¶å‡ºé”™: {e}")
            continue
    
    return torch.stack(images) if images else torch.empty(0)

def compute_lpips(folder1, folder2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¹‹é—´å›¾åƒçš„LPIPSè·ç¦»"""
    print("ğŸ”„ åŠ è½½å›¾åƒç”¨äºLPIPSè®¡ç®—...")
    imgs1 = load_images_for_lpips(folder1)
    imgs2 = load_images_for_lpips(folder2)
    
    if len(imgs1) == 0 or len(imgs2) == 0:
        raise RuntimeError("âŒ LPIPSè®¡ç®—æ—¶æœªæ‰¾åˆ°æœ‰æ•ˆå›¾åƒ")
    
    if len(imgs1) != len(imgs2):
        min_len = min(len(imgs1), len(imgs2))
        imgs1 = imgs1[:min_len]
        imgs2 = imgs2[:min_len]
        print(f"âš ï¸  å›¾åƒæ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨å‰ {min_len} ä¸ªå›¾åƒ")
    
    imgs1 = imgs1.to(device)
    imgs2 = imgs2.to(device)
    
    distances = []
    print("ğŸ”„ è®¡ç®—LPIPSè·ç¦»...")
    
    for i in tqdm(range(len(imgs1))):
        with torch.no_grad():
            d = lpips_model(imgs1[i:i+1], imgs2[i:i+1])
            distances.append(d.item())
    
    avg_distance = sum(distances) / len(distances)
    print(f"âœ… LPIPSå¹³å‡è·ç¦»: {avg_distance:.4f}")
    
    return avg_distance

def compute_fid(folder1, folder2, batch_size=50):
    """è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¹‹é—´çš„FIDåˆ†æ•°"""
    print("ğŸ”„ è®¡ç®—FIDåˆ†æ•°...")
    try:
        fid_value = fid_score.calculate_fid_given_paths(
            [folder1, folder2], 
            batch_size, 
            device, 
            2048
        )
        print(f"âœ… FIDåˆ†æ•°: {fid_value:.2f}")
        return fid_value
    except Exception as e:
        print(f"âŒ FIDè®¡ç®—å‡ºé”™: {e}")
        raise

# å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£
def compute_clip_score(image_folder, prompt_folder):
    """é»˜è®¤ä½¿ç”¨å…³é”®çŸ­è¯­æ–¹æ³•çš„CLIPåˆ†æ•°è®¡ç®—"""
    return compute_clip_score_with_long_text(image_folder, prompt_folder, method="key_phrases")