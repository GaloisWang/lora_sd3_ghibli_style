import torch
import torchvision.transforms as T
import os
from PIL import Image
from tqdm import tqdm
import lpips
from pytorch_fid import fid_score
from transformers import CLIPProcessor, CLIPModel
import torch.nn.functional as F
import torch
import torch.nn.functional as F
from diffusers import StableDiffusion3Pipeline
from transformers import CLIPVisionModel, CLIPImageProcessor
from tqdm import tqdm
from transformers import (
    T5Tokenizer,
    T5EncoderModel,
    CLIPTokenizer,
    CLIPTextModel,
    CLIPTextModelWithProjection,
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# å®šä¹‰æ¨¡å‹æƒé‡æ–‡ä»¶çš„å®Œæ•´æœ¬åœ°è·¯å¾„
local_hf_model_dir = "/home/models/openai--clip-vit-base-patch32/openai--clip-vit-base-patch32/" 


def setup_clip_model(model_path,device):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"CLIPæ¨¡å‹æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}ã€‚è¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶åã€‚")
    # ä½¿ç”¨æ­£ç¡®çš„CLIPæ¨¡å‹å’Œå¤„ç†å™¨
    clip_model = CLIPModel.from_pretrained(model_path).to(device)
    clip_processor = CLIPProcessor.from_pretrained(model_path)
    clip_model.eval()
    print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
    return clip_model,clip_processor

def setup_t5_model(sd3_model_path, device):
    """è®¾ç½®T5æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨"""
    print("ğŸ”„ åŠ è½½SD3æ¨¡å‹ä¸­çš„T5ç¼–ç å™¨...")
    t5_encoder = T5EncoderModel.from_pretrained(
        sd3_model_path,
        subfolder="text_encoder_3",
        torch_dtype=torch.float16,
        use_safetensors=True,
    ).to(device)

    t5_tokenizer = T5Tokenizer.from_pretrained(
            sd3_model_path, subfolder="tokenizer_3"
        )
        
    # åŠ è½½CLIPè§†è§‰ç¼–ç å™¨ç”¨äºå›¾åƒç¼–ç 
    print("ğŸ”„ åŠ è½½CLIPè§†è§‰ç¼–ç å™¨ç”¨äºå›¾åƒç¼–ç ...")
    vision_model = CLIPVisionModel.from_pretrained(local_hf_model_dir).to(device)
    vision_processor = CLIPImageProcessor.from_pretrained(local_hf_model_dir)
    
    # æ¸…ç†ç®¡é“ä»¥èŠ‚çœå†…å­˜
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return t5_encoder, t5_tokenizer, vision_model, vision_processor



def load_images_and_prompts(image_folder, prompt_folder):
    """åŠ è½½å›¾åƒå’Œå¯¹åº”çš„æç¤ºæ–‡æœ¬"""
    images, prompts, names = [], [], []
    
    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    for fname in sorted(image_files):
        base_name = os.path.splitext(fname)[0]
        prompt_path = os.path.join(prompt_folder, f"{base_name}.txt")
        
        if not os.path.exists(prompt_path):
            print(f"âš ï¸  æç¤ºæ–‡ä»¶æœªæ‰¾åˆ°: {fname}, è·³è¿‡è¯¥æ–‡ä»¶")
            continue
            
        try:
            # åŠ è½½å›¾åƒ
            img_path = os.path.join(image_folder, fname)
            img = Image.open(img_path).convert("RGB")
            images.append(img)
            
            # åŠ è½½å¯¹åº” prompt
            with open(prompt_path, 'r', encoding='utf-8') as pf:
                prompt_text = pf.read().strip()
                prompts.append(prompt_text)
                
            names.append(fname)
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {fname} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(images)} ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹")
    return images, prompts, names

def compute_clip_score(image_folder, prompt_folder):
    clip_model,clip_processor = setup_clip_model(local_hf_model_dir,device)
    """è®¡ç®—CLIPç›¸ä¼¼åº¦åˆ†æ•°"""
    images, prompts, names = load_images_and_prompts(image_folder, prompt_folder)
    
    if len(images) == 0:
        raise RuntimeError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒç”¨äºCLIPè¯„åˆ†")
    
    similarities = []
    batch_size = 8  # æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜
    
    print("ğŸ”„ è®¡ç®—CLIPåˆ†æ•°...")
    for i in tqdm(range(0, len(images), batch_size)):
        batch_images = images[i:i+batch_size]
        batch_prompts = prompts[i:i+batch_size]
        
        # ä½¿ç”¨CLIPå¤„ç†å™¨å¤„ç†è¾“å…¥
        inputs = clip_processor(
            text=batch_prompts, 
            images=batch_images, 
            return_tensors="pt", 
            padding=True
        )
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = clip_model(**inputs)
            logits_per_image = outputs.logits_per_image
            
            # è·å–å¯¹è§’çº¿å…ƒç´ ï¼ˆæ¯ä¸ªå›¾åƒä¸å…¶å¯¹åº”æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼‰
            batch_similarities = torch.diag(logits_per_image)
            similarities.extend(batch_similarities.cpu().tolist())
    
    avg_similarity = sum(similarities) / len(similarities)
    print(f"âœ… CLIPå¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    return avg_similarity

def load_images_for_lpips(image_folder):
    """ä¸ºLPIPSåŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸ºå¼ é‡"""
    images = []
    transform = T.Compose([
        T.Resize((256, 256)),  # LPIPSé€šå¸¸ä½¿ç”¨256x256
        T.ToTensor(),
        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # å½’ä¸€åŒ–åˆ°[-1,1]
    ])
    
    image_files = sorted([f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    
    for fname in image_files:
        try:
            img_path = os.path.join(image_folder, fname)
            img = Image.open(img_path).convert("RGB")
            img_tensor = transform(img)
            images.append(img_tensor)
        except Exception as e:
            print(f"âš ï¸  å¤„ç†LPIPSå›¾åƒ {fname} æ—¶å‡ºé”™: {e}")
            continue
    
    return torch.stack(images) if images else torch.empty(0)

def compute_lpips(folder1, folder2):
    # åŠ è½½ LPIPS æ¨¡å‹
    lpips_model = lpips.LPIPS(net='alex').to(device)
    """è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¹‹é—´å›¾åƒçš„LPIPSè·ç¦»"""
    print("ğŸ”„ åŠ è½½å›¾åƒç”¨äºLPIPSè®¡ç®—...")
    imgs1 = load_images_for_lpips(folder1)
    imgs2 = load_images_for_lpips(folder2)
    
    if len(imgs1) == 0 or len(imgs2) == 0:
        raise RuntimeError("âŒ LPIPSè®¡ç®—æ—¶æœªæ‰¾åˆ°æœ‰æ•ˆå›¾åƒ")
    
    if len(imgs1) != len(imgs2):
        min_len = min(len(imgs1), len(imgs2))
        imgs1 = imgs1[:min_len]
        imgs2 = imgs2[:min_len]
        print(f"âš ï¸  å›¾åƒæ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨å‰ {min_len} ä¸ªå›¾åƒ")
    
    imgs1 = imgs1.to(device)
    imgs2 = imgs2.to(device)
    
    distances = []
    print("ğŸ”„ è®¡ç®—LPIPSè·ç¦»...")
    
    for i in tqdm(range(len(imgs1))):
        with torch.no_grad():
            d = lpips_model(imgs1[i:i+1], imgs2[i:i+1])
            distances.append(d.item())
    
    avg_distance = sum(distances) / len(distances)
    print(f"âœ… LPIPSå¹³å‡è·ç¦»: {avg_distance:.4f}")
    
    return avg_distance

def compute_fid(folder1, folder2, batch_size=50):
    """è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¹‹é—´çš„FIDåˆ†æ•°"""
    print("ğŸ”„ è®¡ç®—FIDåˆ†æ•°...")
    try:
        fid_value = fid_score.calculate_fid_given_paths(
            [folder1, folder2], 
            batch_size, 
            device, 
            2048
        )
        print(f"âœ… FIDåˆ†æ•°: {fid_value:.2f}")
        return fid_value
    except Exception as e:
        print(f"âŒ FIDè®¡ç®—å‡ºé”™: {e}")
        raise


def compute_t5_score(image_folder, prompt_folder, sd3_model_path, device='cuda'):
    """è®¡ç®—T5ç›¸ä¼¼åº¦åˆ†æ•°"""
    images, prompts, names = load_images_and_prompts(image_folder, prompt_folder)
    
    if len(images) == 0:
        raise RuntimeError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒç”¨äºT5è¯„åˆ†")
    
    # è®¾ç½®æ¨¡å‹
    t5_encoder, t5_tokenizer, vision_model, vision_processor = setup_t5_model(sd3_model_path, device)
    
    # åˆ›å»ºä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚æ¥å¯¹é½åµŒå…¥ç»´åº¦
    # T5: 4096ç»´ -> CLIP: 768ç»´
    projection_layer = torch.nn.Linear(4096, 768).to(device)
    
    similarities = []
    batch_size = 4  # T5æ¨¡å‹è¾ƒå¤§ï¼Œä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡
    
    print("ğŸ”„ è®¡ç®—T5åˆ†æ•°...")
    for i in tqdm(range(0, len(images), batch_size)):
        batch_images = images[i:i+batch_size]
        batch_prompts = prompts[i:i+batch_size]

        text_inputs = t5_tokenizer(
            batch_prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=1024  # T5å¯ä»¥å¤„ç†æ›´é•¿çš„åºåˆ—
        )
        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}
        
        # ç¼–ç å›¾åƒ - ä½¿ç”¨CLIPè§†è§‰ç¼–ç å™¨
        image_inputs = vision_processor(batch_images, return_tensors="pt")
        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}
        
        with torch.no_grad():
            # è·å–T5æ–‡æœ¬åµŒå…¥
            text_outputs = t5_encoder(**text_inputs)
            # ä½¿ç”¨å¹³å‡æ± åŒ–è·å¾—å¥å­çº§åˆ«çš„è¡¨ç¤º
            text_embeddings = text_outputs.last_hidden_state.mean(dim=1)
            
            # è·å–CLIPå›¾åƒåµŒå…¥
            image_outputs = vision_model(**image_inputs)
            image_embeddings = image_outputs.pooler_output
            
            # å°†T5åµŒå…¥æŠ•å½±åˆ°CLIPåµŒå…¥ç©ºé—´
            text_embeddings_projected = projection_layer(text_embeddings.float())
            
            # æ ‡å‡†åŒ–åµŒå…¥
            text_embeddings_projected = F.normalize(text_embeddings_projected, p=2, dim=-1)
            image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            batch_similarities = torch.sum(text_embeddings_projected * image_embeddings, dim=-1)
            similarities.extend(batch_similarities.cpu().tolist())
    
    avg_similarity = sum(similarities) / len(similarities)
    print(f"âœ… T5å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    return avg_similarity