import os
import json
import torch
from pathlib import Path
from safetensors import safe_open
from safetensors.torch import save_file
from typing import Dict, Any, Optional, List
import re

class ImprovedSD3LoRAConverter:
    def __init__(self):
        # SD3çš„æ ‡å‡†ç›®æ ‡æ¨¡å—
        self.sd3_target_modules = [
            "to_k", "to_q", "to_v", "to_out.0",
            "proj_in", "proj_out", 
            "ff.net.0.proj", "ff.net.2"
        ]
        
        # é”®åæ˜ å°„è§„åˆ™
        self.key_mappings = {
            # ç§»é™¤å¸¸è§å‰ç¼€
            r'^lora_unet_': '',
            r'^lora_te_': 'text_encoder.',
            r'^lora_te1_': 'text_encoder.',
            r'^lora_te2_': 'text_encoder_2.',
            r'^lora_te3_': 'text_encoder_3.',
            
            # æ ‡å‡†åŒ–å±‚å
            r'_transformer_blocks_': '.transformer_blocks.',
            r'_attn_': '.attn.',
            r'_norm_': '.norm.',
            r'_ff_': '.ff.',
            r'_net_': '.net.',
            
            # å¤„ç†ä¸‹åˆ’çº¿åˆ°ç‚¹çš„è½¬æ¢
            r'transformer_blocks_(\d+)_': r'transformer.transformer_blocks.\1.',
            r'x_embedder_': 'transformer.x_embedder.',
            r'y_embedder_': 'transformer.y_embedder.',
            r'context_embedder_': 'transformer.context_embedder.',
            r't_embedder_': 'transformer.t_embedder.',
            r'norm_out_': 'transformer.norm_out.',
            r'proj_out_': 'transformer.proj_out.',
        }
    
    def normalize_key_name(self, key: str) -> str:
        """æ ‡å‡†åŒ–æƒé‡é”®å"""
        normalized = key
        
        # åº”ç”¨æ˜ å°„è§„åˆ™
        for pattern, replacement in self.key_mappings.items():
            normalized = re.sub(pattern, replacement, normalized)
        
        # ç¡®ä¿transformerè·¯å¾„æ­£ç¡®
        if 'transformer_blocks' in normalized and not normalized.startswith('transformer.'):
            normalized = 'transformer.' + normalized
        
        # æ¸…ç†å¤šä½™çš„ç‚¹å’Œä¸‹åˆ’çº¿
        normalized = re.sub(r'\.+', '.', normalized)  # å¤šä¸ªç‚¹å˜ä¸€ä¸ª
        normalized = re.sub(r'_+', '_', normalized)   # å¤šä¸ªä¸‹åˆ’çº¿å˜ä¸€ä¸ª
        normalized = normalized.strip('._')           # ç§»é™¤é¦–å°¾çš„ç‚¹å’Œä¸‹åˆ’çº¿
        
        return normalized
    
    def analyze_original_lora(self, lora_path: str) -> Dict[str, Any]:
        """åˆ†æåŸå§‹LoRAæ–‡ä»¶"""
        analysis = {
            "files": [],
            "tensors": {},
            "statistics": {
                "total_tensors": 0,
                "lora_pairs": 0,
                "alpha_tensors": 0,
                "unique_modules": set()
            }
        }
        
        lora_dir = Path(lora_path)
        safetensor_files = list(lora_dir.glob("*.safetensors"))
        
        for file_path in safetensor_files:
            analysis["files"].append(str(file_path))
            
            with safe_open(file_path, framework="pt", device="cpu") as f:
                for key in f.keys():
                    tensor = f.get_tensor(key)
                    analysis["tensors"][key] = {
                        "shape": list(tensor.shape),
                        "dtype": str(tensor.dtype),
                        "file": str(file_path),
                        "normalized_key": self.normalize_key_name(key)
                    }
                    
                    analysis["statistics"]["total_tensors"] += 1
                    
                    # ç»Ÿè®¡LoRAç»„ä»¶
                    if "lora_A" in key or "lora_a" in key:
                        analysis["statistics"]["lora_pairs"] += 0.5
                    elif "lora_B" in key or "lora_b" in key:
                        analysis["statistics"]["lora_pairs"] += 0.5
                    elif "alpha" in key.lower():
                        analysis["statistics"]["alpha_tensors"] += 1
                    
                    # æå–æ¨¡å—å
                    module_name = self.extract_module_name(key)
                    analysis["statistics"]["unique_modules"].add(module_name)
        
        analysis["statistics"]["unique_modules"] = list(analysis["statistics"]["unique_modules"])
        return analysis
    
    def extract_module_name(self, key: str) -> str:
        """ä»é”®åä¸­æå–æ¨¡å—å"""
        # ç§»é™¤LoRAç‰¹å®šåç¼€
        clean_key = re.sub(r'\.(lora_[AB]|alpha)$', '', key, flags=re.IGNORECASE)
        
        # æå–æœ€åçš„æ¨¡å—å
        parts = clean_key.split('.')
        if len(parts) >= 2:
            return parts[-1]  # è¿”å›æœ€åä¸€ä¸ªéƒ¨åˆ†
        return clean_key
    
    def create_adapter_config(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºé€‚é…å™¨é…ç½®"""
        # ä»åˆ†æç»“æœä¸­ç¡®å®šç›®æ ‡æ¨¡å—
        detected_modules = []
        for module in analysis["statistics"]["unique_modules"]:
            if module in self.sd3_target_modules:
                detected_modules.append(module)
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ ‡å‡†æ¨¡å—ï¼Œä½¿ç”¨åˆ†æå‡ºçš„æ‰€æœ‰æ¨¡å—
        if not detected_modules:
            detected_modules = list(analysis["statistics"]["unique_modules"])
        
        config = {
            "base_model_name_or_path": "/root/autodl-tmp/models/stabilityai--stable-diffusion-3-medium-diffusers/",
            "library_name": "diffusers",
            "peft_type": "LORA",
            "target_modules": detected_modules,
            "task_type": "DIFFUSION",
            "inference_mode": False,
            "r": 16,  # é»˜è®¤rank
            "lora_alpha": 16,  # é»˜è®¤alpha
            "lora_dropout": 0.0,
            "bias": "none",
            "use_rslora": False,
            "use_dora": False
        }
        
        return config
    
    def convert_lora(self, input_path: str, output_path: str, 
                    force_overwrite: bool = False) -> bool:
        """è½¬æ¢LoRAæ¨¡å‹"""
        input_dir = Path(input_path)
        output_dir = Path(output_path)
        
        if output_dir.exists() and not force_overwrite:
            print(f"è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}")
            print("ä½¿ç”¨ force_overwrite=True æ¥è¦†ç›–")
            return False
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ“Š åˆ†æåŸå§‹LoRAæ–‡ä»¶...")
        analysis = self.analyze_original_lora(str(input_dir))
        
        print(f"å‘ç° {analysis['statistics']['total_tensors']} ä¸ªå¼ é‡")
        print(f"å‘ç° {int(analysis['statistics']['lora_pairs'])} ä¸ªLoRAå¯¹")
        print(f"ç›®æ ‡æ¨¡å—: {analysis['statistics']['unique_modules']}")
        
        # è½¬æ¢å¼ é‡
        print("\nğŸ”„ è½¬æ¢å¼ é‡...")
        converted_tensors = {}
        conversion_log = []
        
        for old_key, tensor_info in analysis["tensors"].items():
            new_key = tensor_info["normalized_key"]
            
            # åŠ è½½å¼ é‡
            with safe_open(tensor_info["file"], framework="pt", device="cpu") as f:
                tensor = f.get_tensor(old_key)
            
            converted_tensors[new_key] = tensor
            conversion_log.append(f"{old_key} -> {new_key}")
            print(f"  {old_key} -> {new_key}")
        
        # ä¿å­˜è½¬æ¢åçš„å¼ é‡
        adapter_file = output_dir / "adapter_model.safetensors"
        save_file(converted_tensors, adapter_file)
        print(f"âœ… ä¿å­˜æƒé‡æ–‡ä»¶: {adapter_file}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config = self.create_adapter_config(analysis)
        config_file = output_dir / "adapter_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜é…ç½®æ–‡ä»¶: {config_file}")
        
        # ä¿å­˜è½¬æ¢æ—¥å¿—
        log_file = output_dir / "conversion_log.txt"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write("LoRAè½¬æ¢æ—¥å¿—\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"è¾“å…¥è·¯å¾„: {input_path}\n")
            f.write(f"è¾“å‡ºè·¯å¾„: {output_path}\n")
            f.write(f"æ€»å¼ é‡æ•°: {len(converted_tensors)}\n\n")
            f.write("é”®åè½¬æ¢:\n")
            for log_entry in conversion_log:
                f.write(f"  {log_entry}\n")
        
        print(f"âœ… ä¿å­˜è½¬æ¢æ—¥å¿—: {log_file}")
        
        return True
    
    def validate_converted_lora(self, lora_path: str) -> Dict[str, Any]:
        """éªŒè¯è½¬æ¢åçš„LoRA"""
        validation = {
            "valid": False,
            "issues": [],
            "tensor_count": 0,
            "lora_pairs": 0
        }
        
        lora_dir = Path(lora_path)
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        adapter_file = lora_dir / "adapter_model.safetensors"
        config_file = lora_dir / "adapter_config.json"
        
        if not adapter_file.exists():
            validation["issues"].append("ç¼ºå°‘adapter_model.safetensors")
            return validation
        
        if not config_file.exists():
            validation["issues"].append("ç¼ºå°‘adapter_config.json")
            return validation
        
        # éªŒè¯é…ç½®æ–‡ä»¶
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            required_keys = ["peft_type", "target_modules", "task_type"]
            for key in required_keys:
                if key not in config:
                    validation["issues"].append(f"é…ç½®æ–‡ä»¶ç¼ºå°‘: {key}")
        except Exception as e:
            validation["issues"].append(f"é…ç½®æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        
        # éªŒè¯æƒé‡æ–‡ä»¶
        try:
            with safe_open(adapter_file, framework="pt", device="cpu") as f:
                tensor_names = list(f.keys())
                validation["tensor_count"] = len(tensor_names)
                
                # è®¡ç®—LoRAå¯¹
                lora_a_count = sum(1 for name in tensor_names if "lora_A" in name or "lora_a" in name)
                lora_b_count = sum(1 for name in tensor_names if "lora_B" in name or "lora_b" in name)
                validation["lora_pairs"] = min(lora_a_count, lora_b_count)
                
                if validation["lora_pairs"] == 0:
                    validation["issues"].append("æœªæ‰¾åˆ°æœ‰æ•ˆçš„LoRAæƒé‡å¯¹")
        except Exception as e:
            validation["issues"].append(f"æƒé‡æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        
        validation["valid"] = len(validation["issues"]) == 0
        return validation

# ä½¿ç”¨ç¤ºä¾‹
def main():
    converter = ImprovedSD3LoRAConverter()
    
    # é…ç½®è·¯å¾„
    input_lora = "/home/lora_sd3_train_logs_0524_1600_0525_1600/lora_final_converted/"
    output_lora = "/home/lora_sd3_train_logs_0524_1600_0525_1600/lora_final_converted2/"
    
    print("ğŸš€ å¼€å§‹æ”¹è¿›çš„LoRAè½¬æ¢...")
    
    # è½¬æ¢LoRA
    success = converter.convert_lora(
        input_path=input_lora,
        output_path=output_lora,
        force_overwrite=True
    )
    
    if success:
        print("\nâœ… è½¬æ¢å®Œæˆ!")
        
        # éªŒè¯è½¬æ¢ç»“æœ
        print("\nğŸ” éªŒè¯è½¬æ¢ç»“æœ...")
        validation = converter.validate_converted_lora(output_lora)
        
        if validation["valid"]:
            print("ğŸ‰ éªŒè¯é€šè¿‡!")
            print(f"å¼ é‡æ•°é‡: {validation['tensor_count']}")
            print(f"LoRAå¯¹æ•°: {validation['lora_pairs']}")
        else:
            print("âŒ éªŒè¯å¤±è´¥:")
            for issue in validation["issues"]:
                print(f"  - {issue}")
    else:
        print("âŒ è½¬æ¢å¤±è´¥")
    
    return output_lora if success else None

if __name__ == "__main__":
    converted_path = main()
    if converted_path:
        print(f"\nğŸ“ è½¬æ¢åçš„LoRAè·¯å¾„: {converted_path}")
        print("ç°åœ¨å¯ä»¥ç”¨è¿™ä¸ªè·¯å¾„æµ‹è¯•LoRAåŠ è½½äº†!")